"""æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹å¼•æ“ â€” SimHash + TF-IDF Cosine Similarity + jieba åˆ†è¯"""
import re
import math
from typing import Dict, Any, List, Tuple
from collections import Counter
import hashlib
import logging

logger = logging.getLogger(__name__)

# ğŸ”§ ä¼˜åŒ–ï¼šå°è¯•å¯¼å…¥jiebaï¼Œæå‡ä¸­æ–‡åˆ†è¯è´¨é‡
try:
    import jieba
    jieba.setLogLevel(logging.WARNING)  # æŠ‘åˆ¶jiebaè°ƒè¯•æ—¥å¿—
    HAS_JIEBA = True
    logger.info("âœ… jieba åˆ†è¯å¼•æ“å·²åŠ è½½")
except ImportError:
    HAS_JIEBA = False
    logger.warning("âš ï¸ jieba æœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€åˆ†è¯ï¼ˆå»ºè®® pip install jiebaï¼‰")

# ä¸­æ–‡åœç”¨è¯ï¼ˆé«˜é¢‘æ— æ„ä¹‰è¯ï¼‰
STOP_WORDS = set("""
çš„ äº† åœ¨ æ˜¯ æˆ‘ æœ‰ å’Œ å°± ä¸ äºº éƒ½ ä¸€ ä¸€ä¸ª ä¸Š ä¹Ÿ å¾ˆ åˆ° è¯´ è¦ å» ä½  ä¼š ç€ æ²¡æœ‰
çœ‹ å¥½ è‡ªå·± è¿™ ä»– å¥¹ å®ƒ ä»¬ é‚£ é‡Œ èƒ½ ä¸‹ è¿‡ ä¹ˆ å¤š å¤§ å° äº› ä¹‹ åŠ ä¸ æˆ– ç­‰ å…¶ ä¸­
å¯¹ è€Œ æ‰€ ä»¥ ä¸º è¢« æŠŠ ä» ä½† å¦‚ ä»€ä¹ˆ å¦‚ä½• å› ä¸º æ‰€ä»¥ ç„¶å å…¶ä¸­ è¿™ä¸ª é‚£ä¸ª è¿˜æ˜¯
å¯ä»¥ å·²ç» éœ€è¦ åº”è¯¥ è¿›è¡Œ é€šè¿‡ æ ¹æ® æŒ‰ç…§ å…³äº å¯¹äº ç”±äº ä¸åŒ ç›¸åŒ ä»¥åŠ
æœ¬ è¯¥ é¡¹ ä¸ª æ¡ ä»½ ç§ æ–¹ é¢ æ¬¡ ä»¶ éƒ¨ ç±» ç»„ çº§ å±‚ æ®µ ç«  èŠ‚ æ¬¾ åˆ™ æ¡æ¬¾
å·¥ç¨‹ å»ºè®¾ æ–½å·¥ é¡¹ç›® å•ä½ å…¬å¸ ä¼ä¸š æŠ•æ ‡ æ‹›æ ‡ é‡‡è´­ æ–¹æ¡ˆ æŠ€æœ¯ è´¨é‡ å®‰å…¨
ç®¡ç† æœåŠ¡ æ ‡å‡† è¦æ±‚ è§„å®š è§„èŒƒ åˆåŒ æ–‡ä»¶ ææ–™ è®¾å¤‡ äººå‘˜ è´Ÿè´£ ç»„ç»‡ å®æ–½
""".split())


class ContentSimilarityDetector:
    """
    æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹å™¨
    ä½¿ç”¨ jieba ä¸­æ–‡åˆ†è¯ + SimHash + TF-IDF ä½™å¼¦ç›¸ä¼¼åº¦
    """

    @staticmethod
    def compute_similarity(text_a: str, text_b: str) -> Dict[str, Any]:
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç»¼åˆç›¸ä¼¼åº¦"""
        if not text_a or not text_b:
            return {"score": 0.0, "details": {"error": "Empty text"}, "similar_segments": []}

        # Clean text
        text_a_clean = ContentSimilarityDetector._clean_text(text_a)
        text_b_clean = ContentSimilarityDetector._clean_text(text_b)

        if len(text_a_clean) < 10 or len(text_b_clean) < 10:
            return {"score": 0.0, "details": {"error": "Text too short after cleaning"}, "similar_segments": []}

        # 1. SimHash ç›¸ä¼¼åº¦ (å¿«é€Ÿç²—ç­›)
        simhash_sim = ContentSimilarityDetector._simhash_similarity(text_a_clean, text_b_clean)

        # 2. TF-IDF Cosine ç›¸ä¼¼åº¦ (æ›´ç²¾ç¡®)
        cosine_sim = ContentSimilarityDetector._tfidf_cosine_similarity(text_a_clean, text_b_clean)

        # 3. Jaccard ç›¸ä¼¼åº¦ (è¯çº§åˆ«)
        jaccard_sim = ContentSimilarityDetector._jaccard_similarity(text_a_clean, text_b_clean)

        # 4. æ‰¾å‡ºç›¸ä¼¼æ®µè½
        similar_segments = ContentSimilarityDetector._find_similar_segments(text_a, text_b)

        # ç»¼åˆè¯„åˆ†: åŠ æƒå¹³å‡
        overall_score = (simhash_sim * 0.2 + cosine_sim * 0.5 + jaccard_sim * 0.3)

        return {
            "score": round(overall_score, 4),
            "details": {
                "simhash_similarity": round(simhash_sim, 4),
                "cosine_similarity": round(cosine_sim, 4),
                "jaccard_similarity": round(jaccard_sim, 4),
                "text_a_length": len(text_a),
                "text_b_length": len(text_b),
                "similar_segment_count": len(similar_segments),
                "tokenizer": "jieba" if HAS_JIEBA else "ngram",
            },
            "similar_segments": similar_segments[:20],  # Top 20
        }

    @staticmethod
    def batch_compare(documents: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æ¯”å¯¹å¤šä»½æ–‡æ¡£çš„ä¸¤ä¸¤ç›¸ä¼¼åº¦
        documents: [{"id": "...", "company": "...", "text": "..."}]
        """
        results = []
        n = len(documents)
        for i in range(n):
            for j in range(i + 1, n):
                sim = ContentSimilarityDetector.compute_similarity(
                    documents[i]["text"],
                    documents[j]["text"]
                )
                results.append({
                    "doc_a_id": documents[i]["id"],
                    "doc_b_id": documents[j]["id"],
                    "company_a": documents[i].get("company", ""),
                    "company_b": documents[j].get("company", ""),
                    "score": sim["score"],
                    "details": sim.get("details", {}),
                    "similar_segments": sim.get("similar_segments", []),
                })
        return results

    # ========== Internal Methods ==========

    @staticmethod
    def _clean_text(text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦"""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\u4e00-\u9fff\w\s.,;:!?ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿã€ï¼ˆï¼‰()]', '', text)
        return text.strip()

    @staticmethod
    def _tokenize(text: str) -> List[str]:
        """åˆ†è¯ï¼šä¼˜å…ˆä½¿ç”¨ jiebaï¼Œå›é€€åˆ° n-gram"""
        if HAS_JIEBA:
            # ä½¿ç”¨ jieba ç²¾ç¡®æ¨¡å¼åˆ†è¯ + å»åœç”¨è¯
            words = jieba.lcut(text)
            return [w.strip() for w in words
                    if len(w.strip()) > 1 and w.strip() not in STOP_WORDS
                    and not w.strip().isspace()]
        else:
            # å›é€€ï¼šåŸºäºå­—ç¬¦ n-gram + ç©ºæ ¼åˆ†è¯
            tokens = []
            words = text.split()
            for word in words:
                if re.search(r'[\u4e00-\u9fff]', word):
                    for k in range(len(word) - 1):
                        tokens.append(word[k:k+2])
                else:
                    if len(word) > 1:
                        tokens.append(word.lower())
            return tokens

    @staticmethod
    def _simhash_similarity(text_a: str, text_b: str) -> float:
        """SimHash ç›¸ä¼¼åº¦ (åŸºäºæ±‰æ˜è·ç¦»)"""
        hash_a = ContentSimilarityDetector._compute_simhash(text_a)
        hash_b = ContentSimilarityDetector._compute_simhash(text_b)

        # Hamming distance
        xor = hash_a ^ hash_b
        hamming = bin(xor).count('1')

        # Convert to similarity (64-bit hash)
        return 1.0 - (hamming / 64.0)

    @staticmethod
    def _compute_simhash(text: str, bits: int = 64) -> int:
        """è®¡ç®— SimHash å€¼"""
        tokens = ContentSimilarityDetector._tokenize(text)
        if not tokens:
            return 0
        v = [0] * bits

        for token in tokens:
            h = int(hashlib.md5(token.encode('utf-8')).hexdigest(), 16)
            for i in range(bits):
                bitmask = 1 << i
                if h & bitmask:
                    v[i] += 1
                else:
                    v[i] -= 1

        fingerprint = 0
        for i in range(bits):
            if v[i] > 0:
                fingerprint |= (1 << i)
        return fingerprint

    @staticmethod
    def _tfidf_cosine_similarity(text_a: str, text_b: str) -> float:
        """TF-IDF ä½™å¼¦ç›¸ä¼¼åº¦"""
        tokens_a = ContentSimilarityDetector._tokenize(text_a)
        tokens_b = ContentSimilarityDetector._tokenize(text_b)

        if not tokens_a or not tokens_b:
            return 0.0

        # Build vocabulary
        all_tokens = set(tokens_a) | set(tokens_b)

        # TF vectors
        tf_a = Counter(tokens_a)
        tf_b = Counter(tokens_b)

        # IDF weights (simple: log(2 / df))
        idf = {}
        for t in all_tokens:
            df = (1 if t in tf_a else 0) + (1 if t in tf_b else 0)
            idf[t] = math.log(2.0 / df) + 1.0

        # TF-IDF weighted cosine similarity
        dot_product = sum(tf_a.get(t, 0) * tf_b.get(t, 0) * idf[t] ** 2 for t in all_tokens)
        mag_a = math.sqrt(sum((tf_a.get(t, 0) * idf[t]) ** 2 for t in all_tokens))
        mag_b = math.sqrt(sum((tf_b.get(t, 0) * idf[t]) ** 2 for t in all_tokens))

        if mag_a == 0 or mag_b == 0:
            return 0.0

        return dot_product / (mag_a * mag_b)

    @staticmethod
    def _jaccard_similarity(text_a: str, text_b: str) -> float:
        """Jaccard ç›¸ä¼¼åº¦"""
        tokens_a = set(ContentSimilarityDetector._tokenize(text_a))
        tokens_b = set(ContentSimilarityDetector._tokenize(text_b))

        if not tokens_a and not tokens_b:
            return 0.0

        intersection = tokens_a & tokens_b
        union = tokens_a | tokens_b

        return len(intersection) / len(union) if union else 0.0

    @staticmethod
    def _find_similar_segments(text_a: str, text_b: str, min_length: int = 15) -> List[Dict[str, Any]]:
        """æ‰¾å‡ºç›¸ä¼¼æ®µè½ï¼ˆåŸºäºå¥å­çº§åˆ«æ¯”å¯¹ï¼‰"""
        segments = []

        # Split into sentences
        sents_a = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text_a)
        sents_b = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text_b)

        sents_a = [s.strip() for s in sents_a if len(s.strip()) >= min_length]
        sents_b = [s.strip() for s in sents_b if len(s.strip()) >= min_length]

        # é™åˆ¶æ¯”è¾ƒæ•°é‡é¿å…æ€§èƒ½é—®é¢˜
        max_sents = 100
        sents_a = sents_a[:max_sents]
        sents_b = sents_b[:max_sents]

        for i, sa in enumerate(sents_a):
            for j, sb in enumerate(sents_b):
                sim = ContentSimilarityDetector._tfidf_cosine_similarity(sa, sb)
                if sim > 0.6:  # High sentence-level similarity
                    segments.append({
                        "text_a_segment": sa[:200],
                        "text_b_segment": sb[:200],
                        "similarity": round(sim, 4),
                        "position_a": i,
                        "position_b": j,
                    })

        # Sort by similarity
        segments.sort(key=lambda x: x["similarity"], reverse=True)
        return segments
